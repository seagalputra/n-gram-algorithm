{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas 1\n",
    "# Prediksi Kemunculan Kata dan Kalimat Menggunakan Model Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dwiferdio Seagal Putra (1301154323)\n",
    "\n",
    "Muhammad Adnan Rizqullah (1301154228)\n",
    "\n",
    "Hafid Junur (1103130285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import chain\n",
    "from math import log10, exp\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendefinisian Fungsi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membaca semua file pada folder txt \n",
    "# menjadi satu list kumpulan berita\n",
    "# Input : null\n",
    "# Output : List text berita\n",
    "\n",
    "def get_texts():\n",
    "    f_path = \"article/detikcom/\"\n",
    "    f_names = []\n",
    "    for i in range(0,153):\n",
    "        f_names.append(\"detik_\" + str(i) + \".txt\")\n",
    "\n",
    "    texts = []\n",
    "    for f_name in f_names:\n",
    "        with open(f_path + f_name) as f:\n",
    "            new_text = f.read()\n",
    "            texts.append(new_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi mencari dan mendapatkan substring editor notes\n",
    "# Menerima text yang berisikan editor notes\n",
    "# Mengeluarkan editor notes\n",
    "\n",
    "def get_editor_notes(editor_txt):\n",
    "    match = re.search(r'\\([a-z]+/[a-z]+\\).*', editor_txt)\n",
    "    if match:\n",
    "        editor_notes = match.group()\n",
    "    return editor_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi menghilangkan editor notes dari sebuah text\n",
    "# Input : Text kotor berisikan editor notes\n",
    "# Output : Text bersih/bebas dari editor notes\n",
    "\n",
    "def get_editor_free_text(editor_notes, dirty_txt):\n",
    "    idx = dirty_txt.find(editor_notes)\n",
    "    editor_free = dirty_txt[:idx]\n",
    "    return editor_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prosedur untuk menghilangkan setiap editor notes dari\n",
    "# semua berita pada sebuah list\n",
    "# Input : List kotor berisikan berita kotor (editor notes)\n",
    "# dan List kosong penampung hasil\n",
    "# Output : List bersih berisikan berita bersih dari editor\n",
    "# notes\n",
    "\n",
    "def remove_editor_notes(editor_free_texts, editor_texts):\n",
    "    for index, text in enumerate(editor_texts):\n",
    "        try:\n",
    "            editor_notes = get_editor_notes(text)\n",
    "        except UnboundLocalError:\n",
    "            print(\"Editor notes not found!\")\n",
    "        else:\n",
    "            editor_free_txt = get_editor_free_text(editor_notes, text)\n",
    "            editor_free_texts.append(editor_free_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi mengubah list text menjadi list token\n",
    "# Input : List text berita\n",
    "# Output : List token berita\n",
    "\n",
    "def get_tokens(texts):\n",
    "    list_of_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        list_of_tokens.append(tokens)\n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi menambah <s> dan <\\s> pada setiap token berita \n",
    "# dari list token berita pada setiap kalimat\n",
    "# Input : List token berita\n",
    "# Output : List token berita dengan <s> dan <\\s>\n",
    "\n",
    "def get_sentence_seperator_list(list_of_tokens):\n",
    "    list_of_sentence_tokens = []\n",
    "    for tokens in list_of_tokens:\n",
    "        if (tokens[-1] != \".\") and (tokens[-1] != \"?\") and (tokens[-1] != \"!\"):\n",
    "            tokens.append(\".\")\n",
    "    for tokens in list_of_tokens:\n",
    "        tokens.insert(0, \"<s>\")\n",
    "        for index, token in enumerate(tokens):\n",
    "            if (token == \".\") or (token == \"?\") or (token == \"!\"):\n",
    "                tokens[index] = \"<\\s>\"\n",
    "                tokens.insert(index + 1, \"<s>\")\n",
    "        del tokens[-1]\n",
    "        list_of_sentence_tokens.append(tokens)\n",
    "    return list_of_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memperbaiki bug dari tokenisasi NLTK berupa\n",
    "# angka dan persentasenya yang terpisah\n",
    "# Input : List token berita\n",
    "# Output : list token berita yang diperbaiki\n",
    "\n",
    "def fix_percentage_problem(list_of_sTokens):\n",
    "    list_of_pTokens = []\n",
    "    for tokens in list_of_sTokens:\n",
    "        for index, token in enumerate(tokens):\n",
    "            if token == \"%\":\n",
    "                old_word = tokens[index - 1]\n",
    "                new_word = old_word +\"%\"\n",
    "                tokens[index - 1] = new_word\n",
    "        list_of_pTokens.append(tokens)\n",
    "    return list_of_pTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memperbaiki bug dari kesalahan penulisan\n",
    "# detik.com berupa dua kalimat yang tergabung\n",
    "# dengan \".\"\n",
    "# Input : List token berita\n",
    "# Output : list token berita yang diperbaiki\n",
    "\n",
    "def fix_dot_problem(list_of_pTokens):\n",
    "    list_of_dTokens = []\n",
    "    for tokens in list_of_pTokens:\n",
    "        for index, token in enumerate(tokens):\n",
    "            if \".\" in token:\n",
    "                dot_index = token.find(\".\")\n",
    "                before_dot = token[:dot_index]\n",
    "                after_dot = token[dot_index + 1:]\n",
    "                del tokens[index]\n",
    "                tokens.insert(index, before_dot)\n",
    "                tokens.insert(index + 1, after_dot)\n",
    "                tokens.insert(index + 1, \"<\\s>\")\n",
    "                tokens.insert(index + 2, \"<s>\")\n",
    "        list_of_dTokens.append(tokens)\n",
    "    return list_of_dTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghilangkan token tanda baca dari setiap berita\n",
    "# pada list token berita\n",
    "# Input : List token berita\n",
    "# Output : list token berita tanpa tanda baca\n",
    "\n",
    "def remove_punctuation(list_of_dTokens):\n",
    "    list_of_puncTokens = []\n",
    "    puncts = string.punctuation\n",
    "    puncts += \"''``\"\n",
    "    for tokens in list_of_dTokens:\n",
    "        clean_tokens = [token for token in tokens if token not in puncts]\n",
    "        list_of_puncTokens.append(clean_tokens)\n",
    "    return list_of_puncTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lowercase(list_of_puncTokens):\n",
    "    list_of_cleanT = []\n",
    "    for tokens in list_of_puncTokens:\n",
    "        tmp = []\n",
    "        for token in tokens:\n",
    "            tmp.append(token.lower())\n",
    "        list_of_cleanT.append(tmp)\n",
    "    return list_of_cleanT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi semua preprocessing, menerima tokens hasil tokenisasi NLTK. \n",
    "# Mengeluarkan token bersih, hasil preprocessing\n",
    "\n",
    "def get_clean_tokens(texts):\n",
    "    # 1. Menghilangkan editor notes dari semua berita\n",
    "    editor_free_texts = []\n",
    "    remove_editor_notes(editor_free_texts, texts)\n",
    "    \n",
    "    # 2. Tokenisasi semua teks berita\n",
    "    list_of_tokens = get_tokens(editor_free_texts)\n",
    "    \n",
    "    # 3. Menambahkan <s> dan <\\s> pada setiap kalimat\n",
    "    list_of_sTokens = get_sentence_seperator_list(list_of_tokens) \n",
    "\n",
    "    # 4. Menyatukan dua token terpisah berupa angka dan persentase yang\n",
    "    # Terpisah setelah tokenisasi NLTK. Seperti : ['25', %] -> ['25%']\n",
    "    list_of_pTokens = fix_percentage_problem(list_of_sTokens)\n",
    "            \n",
    "    # 5. Memisahkan dua kata yang menjadi satu karena salah tulis pada\n",
    "    # Awal dan akhir kalimat. Lalu menambahkan <s> dan <\\s> pada\n",
    "    # Tempatnya. Seperti : [\"DNS.Parahnya\"] -> [\"DNS\", \"<\\s>\",\"<s>\", \"Parahnya\"]\n",
    "    list_of_dTokens = fix_dot_problem(list_of_pTokens)\n",
    "    \n",
    "    # 6. Menghilangkan tanda baca\n",
    "    list_of_puncTokens = remove_punctuation(list_of_dTokens)\n",
    "    \n",
    "    # 7. Mengubah semua kata menjadi lowercase\n",
    "    list_of_cleanT = make_lowercase(list_of_puncTokens)\n",
    "    return list_of_cleanT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi membuat bigram dan countnya. Menerima token bersih berita\n",
    "# Mengeluarkan bigram matrix\n",
    "\n",
    "def get_bigram_matrix(clean_tokens):\n",
    "    bigrams = []\n",
    "    for i in range(len(clean_tokens) - 1 ):\n",
    "        bigrams.append((clean_tokens[i], clean_tokens[i + 1]))\n",
    "    bi_matrix = Counter(bigrams)\n",
    "    return(bi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(clean_tokens):\n",
    "    bigrams = []\n",
    "    for i in range(len(clean_tokens)-1):\n",
    "        bigrams.append((clean_tokens[i], clean_tokens[i+1]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi menghitung probabilitas kemunculan sebuah kalimat\n",
    "# Input : kalimat\n",
    "# Output : probilitas dalam decimal\n",
    "# mengeluarkan nilai 0 jika terdapat kata pada kalimat\n",
    "# yang tak dikenal. Dengan Implementasi Laplace\n",
    "\n",
    "def get_sentence_probability(sentence):\n",
    "    test = word_tokenize(sentence.lower())\n",
    "    punct = string.punctuation\n",
    "    test_clean = [token for token in test if token not in punct]\n",
    "    test_bigram = [(test_clean[i], test_clean[i+1]) for i in range(len(test_clean) - 1)]\n",
    "    start = \"<s>\"\n",
    "    end = \"<\\s>\"\n",
    "    test_bigram.insert(0, (start, test_bigram[0][0]))\n",
    "    test_bigram.append((test_bigram[-1][-1], end))\n",
    "\n",
    "    probability = 0\n",
    "    log_probability = 0\n",
    "    # Implementasi prediksi kumunculan kalimat + Laplace\n",
    "    for tup in test_bigram:\n",
    "        try:\n",
    "            pembagi = count_token[tup[0]] + len(count_token)\n",
    "        except KeyError:\n",
    "            pembagi = len(count_token)\n",
    "            pembilang = 1\n",
    "            log_probability = log10(pembilang/pembagi)\n",
    "        else:\n",
    "            try:\n",
    "                pembilang = count_bigrams[tup] + 1\n",
    "            except KeyError:\n",
    "                pembilang = 1\n",
    "                log_probability = log10(pembilang/pembagi)\n",
    "            else:\n",
    "                log_probability = log10(pembilang/pembagi)\n",
    "    probability = exp(log_probability)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(word, prev_word, length_corpus, list_token, list_bigrams):\n",
    "    '''\n",
    "    Fungsi untuk menghitung probabilitas dari suatu kalimat\n",
    "    '''\n",
    "    count_token = dict(Counter(list_token))\n",
    "    count_bigrams = dict(Counter(list_bigrams))\n",
    "    \n",
    "    try:\n",
    "        return (count_bigrams[word] + 1) / (count_token[prev_word] + length_corpus)    \n",
    "    except KeyError:\n",
    "        try:\n",
    "            return 1 / (count_token[prev_word] + length_corpus)\n",
    "        except Exception:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fungsi memprediksi kata selanjutnya\n",
    "# Input : kalimat\n",
    "# Output : kata selanjutnya\n",
    "\n",
    "def get_next_word(sentence): \n",
    "    punct = string.punctuation.replace(\"%\", \"\")\n",
    "    clean_token = [token for token in word_tokenize(sentence.lower()) if token not in punct]\n",
    "    last_word = clean_token[-1]\n",
    "\n",
    "    max_value = -1\n",
    "    word_prediction = \"\"\n",
    "    idx = 0\n",
    "    for tup, count in count_bigrams.items():\n",
    "        if last_word == tup[0]:\n",
    "            if tup[1] != \"<\\s>\":\n",
    "                if max_value < count:\n",
    "                    max_value = count\n",
    "                    word_prediction = tup[1]\n",
    "    if word_prediction == \"\":\n",
    "        return \"Error, kata terakhir tak dikenal\"\n",
    "    else:\n",
    "        return word_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembangunan Model Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# 1. Menghilangkan catatan editor dari setiap berita\n",
    "# 2. Tokensiasi semua berita\n",
    "# 3. Menambah <s> dan <\\s>\n",
    "# 4. Mengatasi bug karena library NLTK\n",
    "# 5. Mengatasi bug karena kesalahan penulisan detik.com\n",
    "# 6. Menghilangkan tanda baca\n",
    "# 7. Mengubah semua token menjadi lowercase\n",
    "\n",
    "texts = get_texts()\n",
    "list_of_cleanT = get_clean_tokens(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi 2-D list ke 1-D list dan membuat Bigram Matrix\n",
    "\n",
    "list_final_tokens = list(chain.from_iterable(list_of_cleanT))\n",
    "bigram_matrix = get_bigram_matrix(list_final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkankan count setiap token dan tuple\n",
    "\n",
    "count_token = dict(Counter(list_final_tokens))\n",
    "count_bigrams = dict(bigram_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung perplexity dari suatu kalimat\n",
    "# Input : Kalimat\n",
    "# Ouput : Perplexity dari kalimat tersebut\n",
    "def get_perplexity(sentences):\n",
    "    n_root = lambda number, root: number ** (1/root)\n",
    "    words = word_tokenize(sentences)\n",
    "    bigram = get_bigram(words)\n",
    "    length_corpus = len(count_token)\n",
    "\n",
    "    probability = [get_probability(bigram[i], bigram[i][0], length_corpus, list_final_tokens, bigram_matrix) for i in range(len(bigram))]\n",
    "\n",
    "    non_zero_probability = list(filter(lambda x: x != 0, probability))\n",
    "    inv_probability = []\n",
    "    for i in range(len(non_zero_probability)):\n",
    "        inv_probability.append(1/non_zero_probability[i])\n",
    "        temp_perplexity = reduce(lambda x, y: x*y, inv_probability)\n",
    "        perplexity = n_root(temp_perplexity, len(words))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325.46056398758174\n"
     ]
    }
   ],
   "source": [
    "# Menghitung perplexity untuk kalimat pada pengujian probabilitas kemunculan sebuah kalimat\n",
    "sentence = \"Setelah itu barulah pihak bank mempolisikan Qisheng\"\n",
    "perplexity = get_perplexity(sentence)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627.7278990120345\n"
     ]
    }
   ],
   "source": [
    "# Menghitung perplexity untuk kalimat pada pengujian probabilitas kemunculan sebuah kalimat\n",
    "sentence = \"WhatsApp akan mengendus adanya tautan yang dianggap mencurigakan pada chat\"\n",
    "perplexity = get_perplexity(sentence)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.34306404099456\n"
     ]
    }
   ],
   "source": [
    "# Menghitung perplexity untuk kalimat pada pengujian probabilitas kemunculan sebuah kalimat\n",
    "sentence = \"Keberadaan sebuah alat untuk meretas iPhone secara remote terungkap\"\n",
    "perplexity = get_perplexity(sentence)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.47042945362318\n"
     ]
    }
   ],
   "source": [
    "# Menghitung perplexity untuk kalimat pada pengujian probabilitas kemunculan sebuah kalimat\n",
    "sentence = \"Pemilik rekening diduga menyuntikkan transaksi\"\n",
    "perplexity = get_perplexity(sentence)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.65439791566726\n"
     ]
    }
   ],
   "source": [
    "# Menghitung perplexity untuk kalimat pada pengujian probabilitas kemunculan sebuah kalimat\n",
    "sentence = \"Polisi menghentikan kasus pasar saham\"\n",
    "perplexity = get_perplexity(sentence)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediksi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probabilitas Kemunculan Sebuah Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03431912917479203\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Menghitung probabilitas kemunculan sebuah kalimat\n",
    "Kasus Uji 1\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir sesuai korpus. Mau diperiksa kemampuan dalam memprediksi kata yang dikenal persis\n",
    "\n",
    "'''\n",
    "sentence = \"Setelah itu barulah pihak bank mempolisikan Qisheng\"\n",
    "probabilitas = get_sentence_probability(sentence)\n",
    "print(probabilitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028747995206027978\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Menghitung probabilitas kemunculan sebuah kalimat\n",
    "Kasus Uji 2\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir sesuai korpus. Mau diperiksa kemampuan dalam memprediksi kata yang dikenal persis\n",
    "\n",
    "'''\n",
    "sentence = \"WhatsApp akan mengendus adanya tautan yang dianggap mencurigakan pada chat\"\n",
    "probabilitas = get_sentence_probability(sentence)\n",
    "print(probabilitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028785090479781318\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Menghitung probabilitas kemunculan sebuah kalimat\n",
    "Kasus Uji 3\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir sesuai korpus. Mau diperiksa kemampuan dalam memprediksi kata yang dikenal persis\n",
    "\n",
    "'''\n",
    "sentence = \"Keberadaan sebuah alat untuk meretas iPhone secara remote terungkap\"\n",
    "probabilitas = get_sentence_probability(sentence)\n",
    "print(probabilitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03431280307578489\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Menghitung probabilitas kemunculan sebuah kalimat\n",
    "Kasus Uji 4\n",
    "\n",
    "Alasan pemilihan : \n",
    "Kalimat dipilih dari kata yang ada di korpus dan disusun sesuai dengan kaidah berbahasa\n",
    "\n",
    "'''\n",
    "sentence = \"Pemilik rekening diduga menyuntikkan transaksi\"\n",
    "probabilitas = get_sentence_probability(sentence)\n",
    "print(probabilitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028772707828094803\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Menghitung probabilitas kemunculan sebuah kalimat\n",
    "Kasus Uji 5\n",
    "\n",
    "Alasan pemilihan : \n",
    "Kalimat dipilih dari kata yang ada di korpus dan disusun sesuai dengan kaidah berbahasa\n",
    "\n",
    "'''\n",
    "sentence = \"Polisi menghentikan kasus pasar saham\"\n",
    "probabilitas = get_sentence_probability(sentence)\n",
    "print(probabilitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediksi Kemunculan Kata Berikutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qisheng\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 1\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "sentence = \"Setelah itu barulah pihak bank mempolisikan\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celah\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 2\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"WhatsApp akan mengendus adanya\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 3\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Keberadaan sebuah alat untuk meretas\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informasi\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 4\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Kloningan yang menyamar sebagai\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yang\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 5\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Aplikasi dan game kloningan\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sebuah\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 6\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Pemilik rekening diduga menyuntikkan\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ini\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 7\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Polisi menghentikan kasus\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saat\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 8\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Pembobol digrebek polisi\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sistem\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 9\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "sentence = \"Beberapa penipuan menjalankan\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prediksi kemunculan suatu kata berikutnya\n",
    "Kasus Uji 10\n",
    "\n",
    "Alasan pemilihan : \n",
    "Ini kalimat persis dari awal hingga akhir\n",
    "sesuai korpus. Juga kata mempolisikan hanya diikuti oleh \"qisheng\".\n",
    "Mau diperiksa kemampuan dalam memprediksi kalimat yang sama persis\n",
    "dengan korpus. Output harus \"qisheng\"\n",
    "\n",
    "'''\n",
    "\n",
    "sentence = \"Konten-konten pada imessage tersimpan\"\n",
    "word_prediction = get_next_word(sentence)\n",
    "print(word_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
